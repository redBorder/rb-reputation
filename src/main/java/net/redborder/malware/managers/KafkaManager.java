package net.redborder.malware.managers;


import com.fasterxml.jackson.core.type.TypeReference;
import com.fasterxml.jackson.databind.ObjectMapper;
import kafka.common.KafkaException;
import kafka.consumer.Consumer;
import kafka.consumer.ConsumerConfig;
import kafka.consumer.ConsumerIterator;
import kafka.consumer.KafkaStream;
import kafka.javaapi.consumer.ConsumerConnector;
import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;
import net.redborder.malware.config.Config;
import net.redborder.malware.config.ConfigException;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;


import java.io.IOException;
import java.util.*;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.LinkedBlockingQueue;

public class KafkaManager {

    final Logger log = LoggerFactory.getLogger(KafkaManager.class);

    private static Producer<String, String> producer;

    private static ProducerConfig producerConfig;

    private static Config config;

    private static ConsumerConnector consumer;

    private ExecutorService executor;

    private LinkedBlockingQueue <Map<String, Object>> queue;

    public KafkaManager(Config conf, LinkedBlockingQueue<Map<String, Object>> refQueue) throws ConfigException {
        config = conf;
        queue = refQueue;

        init(config.getProperties());
    }

    private static void init(Properties props) throws ConfigException {

        producerConfig = new ProducerConfig(props);

        producer = new Producer<String, String>(producerConfig);

        consumer = Consumer.createJavaConsumerConnector(new ConsumerConfig(config.getProperties()));

    }

    public void consumeFromTopic(String topic, int threads) {

        Map<String, Integer> topicCountMap = new HashMap<String, Integer>();

        topicCountMap.put(topic, new Integer(threads));

        Map<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumer.createMessageStreams(topicCountMap);

        List<KafkaStream<byte[], byte[]>> streams = consumerMap.get(topic);

        executor = Executors.newFixedThreadPool(threads);

        int threadNumber = 0;

        for (final KafkaStream stream : streams) {
            executor.submit(new ConsumeMessages(stream, queue));
            threadNumber++;
        }
    }

    public void sendMessage(String topic, String key, String message) {

        KeyedMessage<String, String> msg = new KeyedMessage<String, String>(topic, key, message);

        producer.send(msg);
    }

    public void sendMessage(String topic, String message) {

        KeyedMessage<String, String> msg = new KeyedMessage<String, String>(topic, String.valueOf(null), message);

        producer.send(msg);
    }

    public void closeProducer() {
        try {
            producer.close();
        } catch (KafkaException e) {
            e.printStackTrace();
        }
    }


    private class ConsumeMessages implements Runnable {

        private ObjectMapper _mapper;
        private KafkaStream m_stream;
        private LinkedBlockingQueue<Map<String, Object>> queue = null;

        public ConsumeMessages(KafkaStream a_stream, LinkedBlockingQueue<Map<String, Object>> refQueue) {
            _mapper = new ObjectMapper();
            m_stream = a_stream;
            queue = refQueue;
        }

        @Override
        public void run() {

            ConsumerIterator<byte[], byte[]> it = m_stream.iterator();

            while (it.hasNext()) {
                try {
                    Map<String, Object> mapJson = _mapper.readValue(it.next().message(), new TypeReference<HashMap<String, Object>>() {});

                    mapJson.remove("cuckoo_before");
                    mapJson.put("action", "create");

                    queue.put(mapJson);
                } catch (IOException e) {
                    e.printStackTrace();
                } catch (InterruptedException e) {
                    e.printStackTrace();
                }
            }
        }

    }
}