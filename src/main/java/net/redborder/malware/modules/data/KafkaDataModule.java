package net.redborder.malware.modules.data;

import kafka.consumer.Consumer;
import kafka.consumer.ConsumerConfig;
import kafka.consumer.ConsumerIterator;
import kafka.consumer.KafkaStream;
import kafka.javaapi.consumer.ConsumerConnector;
import net.redborder.malware.config.Config;
import org.codehaus.jackson.map.ObjectMapper;
import org.codehaus.jackson.type.TypeReference;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.util.*;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;
import java.util.concurrent.TimeUnit;

public class KafkaDataModule extends AbstractDataModule{

    private static final Logger log = LoggerFactory.getLogger(KafkaDataModule.class);

    private ConsumerConnector consumerConnector;

    private LinkedList<Map<String, Object>> incrementalList;

    private int lastIndex = 0;

    private Timer execTimer;

    private ExecutorService consumerThread;

    public KafkaDataModule(Config conf){
        super(conf);
    }

    @Override
    public Object getFullDataImpl() {
        return null;
    }

    @Override
    public Object getIncrementalImpl() {

        lastIndex = incrementalList.size();

        return incrementalList.clone();
    }

    @Override
    public Object doQueryImpl() {
        return null;
    }

    @Override
    public void prepareQueryImpl(Object parameter) {
    // nothing
    }

    @Override
    public void initImpl() {

        log.info("Init Kafka data module");

        incrementalList = new LinkedList<>();
        consumerConnector = Consumer.createJavaConsumerConnector(new ConsumerConfig(config.getProperties()));

        execTimer = new Timer();
        Map<String, Integer> topicCountMap = new HashMap<>();
        topicCountMap.put("rb_malware", 1);

        Map<String, List<KafkaStream<byte[], byte[]>>> consumerMap = consumerConnector.createMessageStreams(topicCountMap);
        List<KafkaStream<byte[], byte[]>> streams = consumerMap.get("rb_malware");

        execTimer.scheduleAtFixedRate(new GenerateIncrementalList(), 0L, TimeUnit.SECONDS.toMillis(10));

        consumerThread = Executors.newSingleThreadExecutor();
        consumerThread.submit(new ConsumerFromTopic(streams.get(0)));

    }

    @Override
    public void shutdownImpl() {
        log.info("Shutdown Kafka data module ...");
        consumerConnector.shutdown();
        execTimer.cancel();
        consumerThread.shutdown();
        log.info("Done!");
    }

    @Override
    public String getDataModuleName() {
        return "Kafka";
    }

    private class GenerateIncrementalList extends TimerTask{

        @Override
        public void run() {

            if(lastIndex < incrementalList.size()){
                incrementalList.subList(0, lastIndex).clear();
            }

        }
    }

    private class ConsumerFromTopic implements Runnable{

        private KafkaStream a_stream;
        private ObjectMapper _mapper;

        public ConsumerFromTopic(KafkaStream m_stream){
            this.a_stream = m_stream;
            _mapper = new ObjectMapper();
        }

        @Override
        public void run() {
            ConsumerIterator<byte[], byte[]> it = a_stream.iterator();

            while(it.hasNext()) {
                try {

                    Map<String, Object> mapJson = _mapper.readValue(it.next().message(), new TypeReference<HashMap<String, Object>>() {
                    });

                    mapJson.remove("cuckoo_before");
                    mapJson.put("action", "create");

                    incrementalList.push(mapJson);

                } catch (IOException e) {
                    e.printStackTrace();
                }

            }
        }

    }
}
